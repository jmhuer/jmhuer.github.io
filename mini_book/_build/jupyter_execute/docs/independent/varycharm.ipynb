{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VaryCHarm: A Method to Automatically Vary the Complexity of Harmonies in Music\n",
    "\n",
    " <sub> <a href=\"https://jmhuer.github.io/mini_book/_build/html/docs/portfolio.html\" style=\"color: red; text-decoration: underline;text-decoration-style: dotted;\">‚Üê Back to Projects</a> </sub>\n",
    "\n",
    "<img src=\"../../../../images/varycharm.png\" align=\"center\"/>\n",
    "\n",
    "<br>\n",
    "\n",
    " In this post we define the task of varying the harmonic complexity of music automatically along with a baseline method in accordance to music theory. In addition, we propose metrics for evaluation. Experiment to show that our proposed method outperforms other baseline methods.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "## Introduction\n",
    "---\n",
    "\n",
    "Music complexity can vary but trained and untrained listeners are still able to recognize the original music. In this paper, we first formulate the problem of varying music complexity and propose a method to preserve general harmonic structure and melody while varying the number of notes. To do this, we find a compressed representation of pitch while simultaneously training on symbolic chord predictions. We test different pitch Autoencoders with various sparsity constraints, and evaluate our results by plotting chord recognition accuracy with increas- ing and decreasing number of notes, observations in relation to music theory, and by analysing absolute and relative musical features with a probabilistic framework.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Problem Defenition\n",
    "---\n",
    "\n",
    "\n",
    "It is clear automatically varying the complexity of music has valuable applications. But how do we approach this problem without supervision? In words, we want to add or remove notes without diverging too much from the original \"feeling\" of music. In math we write:\n",
    "> **The Problem of Varying Harmonic Information**:\n",
    ">\n",
    ">We denote Symbolic music information as piano rools where the input is information with a fixed history length $H$ as a matrix $X_t \\triangleq x_{t-H:t} \\in \\{0,1\\}^{P \\times H}$. For simplicity, we denote $\\mathcal{X} = \\{0,1\\}^{P \\times H}$ as the input space.\n",
    ">\n",
    ">Then the goal is to learn a mapping $f_\\theta(X_t; \\eta) \\rightarrow \\hat{X}_t \\in \\mathcal{X}$ parameterized by $\\theta$ such that $\\hat{X}_t$ summarizes (or further ornament) the original piece of music $X_t$, given a hyper-parameter $\\eta \\in [0,1]$ that controls the sparsity level of $\\hat{X}_t$. More specifically, we consider the following optimization:\n",
    ">\n",
    ">$$  \\min_{\\theta} \\mathcal{D}\\bigg(f_\\theta(X_t; \\eta),~X_t\\bigg) ~~\\text{s.t.}~~||f_\\theta(X_t; \\eta)||_0 \\leq \\eta HP.$$\n",
    ">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## How do we solve this problem?\n",
    "---\n",
    "\n",
    "\n",
    "In order to reconstruct pitch vectors with the extra criteria of maintaining theoriginal chord/harmonic functionality, we propose a combined loss of MSE onpitch vector reconstruction and Cross Entropy on symbolic chord targets. Thereconstruction should be such that we do not lose the ability to map the originalchords but the information bottleneck serves to generalize pitch functionality\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Results\n",
    "---\n",
    "\n",
    "### Example 1\n",
    "<img src=\"../../../../images/example1a-1.png\" width=\"70%\"  align=\"center\"/>\n",
    "<div align=\"center\">\n",
    "<audio controls>\n",
    "  <source src=\"../../../../audio/example1a-1.wav\" type=\"audio/wav\">\n",
    "Your browser does not support the audio element.\n",
    "</audio> </div>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"../../../../images/example1a-2.png\" width=\"70%\"  align=\"center\"/>\n",
    "<div align=\"center\">\n",
    "<audio controls>\n",
    "  <source src=\"../../../../audio/example1a-2.wav\" type=\"audio/wav\">\n",
    "Your browser does not support the audio element.\n",
    "</audio> </div>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"../../../../images/example1a-3.png\" width=\"70%\"  align=\"center\"/>\n",
    "<div align=\"center\">\n",
    "<audio controls>\n",
    "  <source src=\"../../../../audio/example1a-3.wav\" type=\"audio/wav\">\n",
    "Your browser does not support the audio element.\n",
    "</audio> </div>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "---\n",
    "### Example 2\n",
    "<img src=\"../../../../images/example2a-1.png\" width=\"70%\"  align=\"center\"/>\n",
    "<div align=\"center\">\n",
    "<audio controls>\n",
    "  <source src=\"../../../../audio/example2a-1.wav\" type=\"audio/wav\">\n",
    "Your browser does not support the audio element.\n",
    "</audio> </div>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"../../../../images/example2a-2.png\" width=\"70%\"  align=\"center\"/>\n",
    "<div align=\"center\">\n",
    "<audio controls>\n",
    "  <source src=\"../../../../audio/example2a-2.wav\" type=\"audio/wav\">\n",
    "Your browser does not support the audio element.\n",
    "</audio> </div>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"../../../../images/example2a-3.png\" width=\"70%\"  align=\"center\"/>\n",
    "<div align=\"center\">\n",
    "<audio controls>\n",
    "  <source src=\"../../../../audio/example2a-3.wav\" type=\"audio/wav\">\n",
    "Your browser does not support the audio element.\n",
    "</audio> </div>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "---\n",
    "### Example 3 - Added Notes as Strings\n",
    "<img src=\"../../../../images/example3a-1.png\" width=\"70%\"  align=\"center\"/>\n",
    "<div align=\"center\">\n",
    "<audio controls>\n",
    "  <source src=\"../../../../audio/example3a-1.wav\" type=\"audio/wav\">\n",
    "Your browser does not support the audio element.\n",
    "</audio> </div>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"../../../../images/example3a-2.png\" width=\"70%\"  align=\"center\"/>\n",
    "<div align=\"center\">\n",
    "<audio controls>\n",
    "  <source src=\"../../../../audio/example3a-2.wav\" type=\"audio/wav\">\n",
    "Your browser does not support the audio element.\n",
    "</audio> </div>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"../../../../images/example3a-3.png\" width=\"70%\"  align=\"center\"/>\n",
    "<div align=\"center\">\n",
    "<audio controls>\n",
    "  <source src=\"../../../../audio/example3a-3.wav\" type=\"audio/wav\">\n",
    "Your browser does not support the audio element.\n",
    "</audio> </div>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "---\n",
    "## Conclusion\n",
    "Our results indicate the end-to-end autoencoder-BiLSTM Lifetime method outperforms a simple music theory baseline, and a regular auto encoder according to the metrics discussed. The current method does have a few limi- tations. Namely we are compressing pitch information and most of the added embellishments are added vertically and depend on build on the existing rhythm. However, we believe this method and evaluation scheme provides some ground work for exploring rhythmic components to potentially be extended."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "source_map": [
   10
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 4
}